{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install facenet-pytorch opencv-python numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLJUT9tby57S",
        "outputId": "79fe2fad-b498-495e-f7fe-0dcba0c61c40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZx0_9RKyri7",
        "outputId": "ed2b7df2-4429-4f31-8eb1-2834f16adc22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__talking_against_wall.mp4...\n",
            " Skipping already processed: 01__talking_against_wall\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__kitchen_pan.mp4...\n",
            " Skipping already processed: 01__kitchen_pan\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__exit_phone_room.mp4...\n",
            " Skipping already processed: 01__exit_phone_room\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__hugging_happy.mp4...\n",
            " Skipping already processed: 01__hugging_happy\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__outside_talking_pan_laughing.mp4...\n",
            " Skipping already processed: 01__outside_talking_pan_laughing\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__podium_speech_happy.mp4...\n",
            " Skipping already processed: 01__podium_speech_happy\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__kitchen_still.mp4...\n",
            " Skipping already processed: 01__kitchen_still\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__meeting_serious.mp4...\n",
            " Skipping already processed: 01__meeting_serious\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__outside_talking_still_laughing.mp4...\n",
            " Skipping already processed: 01__outside_talking_still_laughing\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/real/01__secret_conversation.mp4...\n",
            " Skipping already processed: 01__secret_conversation\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_12__outside_talking_pan_laughing__TNI7KUZ6.mp4...\n",
            " Skipping already processed: 01_12__outside_talking_pan_laughing__TNI7KUZ6\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_03__hugging_happy__ISF9SP4G.mp4...\n",
            " Skipping already processed: 01_03__hugging_happy__ISF9SP4G\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_11__secret_conversation__4OJNJLOO.mp4...\n",
            " Skipping already processed: 01_11__secret_conversation__4OJNJLOO\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_03__talking_against_wall__JZUXXFRB.mp4...\n",
            " Skipping already processed: 01_03__talking_against_wall__JZUXXFRB\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_02__walk_down_hall_angry__YVGY8LOK.mp4...\n",
            " Skipping already processed: 01_02__walk_down_hall_angry__YVGY8LOK\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_11__talking_against_wall__9229VVZ3.mp4...\n",
            " Skipping already processed: 01_11__talking_against_wall__9229VVZ3\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_03__podium_speech_happy__480LQD1C.mp4...\n",
            " Skipping already processed: 01_03__podium_speech_happy__480LQD1C\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_02__outside_talking_still_laughing__YVGY8LOK.mp4...\n",
            " Skipping already processed: 01_02__outside_talking_still_laughing__YVGY8LOK\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_11__walking_outside_cafe_disgusted__FAFWDR4W.mp4...\n",
            " Skipping already processed: 01_11__walking_outside_cafe_disgusted__FAFWDR4W\n",
            "Preprocessing /content/drive/MyDrive/ISTVT Dataset/fake/01_11__meeting_serious__9OM3VE0Y.mp4...\n",
            " Skipping already processed: 01_11__meeting_serious__9OM3VE0Y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n",
            "Epoch 1/5 [Train]:   0%|          | 1/1866 [01:39<51:42:27, 99.81s/it, loss=0.6985]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from facenet_pytorch import MTCNN\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "\n",
        "class FacePreprocessor:\n",
        "    def __init__(self, device='cuda:0' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        \"\"\"Process a single frame: detect, align, and crop face\"\"\"\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        boxes, _, landmarks = self.mtcnn.detect(frame_rgb, landmarks=True)\n",
        "\n",
        "        if boxes is None or len(boxes) == 0:\n",
        "            return None\n",
        "\n",
        "        # Get nose tip (3rd landmark point)\n",
        "        nose_tip = landmarks[0][2]\n",
        "\n",
        "        # Calculate face size (1.25x max dimension)\n",
        "        x1, y1, x2, y2 = boxes[0]\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        max_dim = max(w, h) * 1.25\n",
        "\n",
        "        # Calculate new bounding box centered at nose tip\n",
        "        x_center, y_center = nose_tip\n",
        "        x1_new = max(0, int(x_center - max_dim / 2))\n",
        "        y1_new = max(0, int(y_center - max_dim / 2))\n",
        "        x2_new = min(frame.shape[1], int(x_center + max_dim / 2))\n",
        "        y2_new = min(frame.shape[0], int(y_center + max_dim / 2))\n",
        "\n",
        "        # Crop face region\n",
        "        face = frame_rgb[y1_new:y2_new, x1_new:x2_new]\n",
        "        if face.size == 0:\n",
        "            return None\n",
        "\n",
        "        # Resize to 300x300 and convert back to BGR\n",
        "        face = cv2.resize(face, (300, 300))\n",
        "        return cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "class VideoProcessor:\n",
        "    def __init__(self, num_frames=270):\n",
        "        self.num_frames = num_frames\n",
        "        self.face_preprocessor = FacePreprocessor()\n",
        "\n",
        "    def extract_frames(self, video_path):\n",
        "        \"\"\"Extract and process frames from a video\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return []\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_step = max(1, total_frames // self.num_frames)\n",
        "\n",
        "        frames = []\n",
        "        for i in range(self.num_frames):\n",
        "            # Read frame at calculated position\n",
        "            pos = min(i * frame_step, total_frames - 1)\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Process frame\n",
        "            processed = self.face_preprocessor.process_frame(frame)\n",
        "            if processed is not None:\n",
        "                frames.append(processed)\n",
        "\n",
        "        cap.release()\n",
        "        return np.array(frames)  # (T, 300, 300, 3)\n",
        "\n",
        "\n",
        "class SequenceGenerator:\n",
        "    def __init__(self, seq_length=6):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def create_sequences(self, frames):\n",
        "        \"\"\"Create consecutive frame sequences\"\"\"\n",
        "        sequences = []\n",
        "        for i in range(len(frames) - self.seq_length + 1):\n",
        "            sequence = frames[i:i+self.seq_length]\n",
        "            sequences.append(sequence)\n",
        "        return np.array(sequences)  # (N, T, H, W, C)\n",
        "\n",
        "    def normalize(self, sequence):\n",
        "        \"\"\"Normalize sequence for model input\"\"\"\n",
        "        # Convert to [0, 1] range\n",
        "        sequence = sequence.astype(np.float32) / 255.0\n",
        "\n",
        "        # ImageNet normalization\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        sequence = (sequence - mean) / std\n",
        "\n",
        "        # Change to (T, C, H, W)\n",
        "        return np.transpose(sequence, (0, 3, 1, 2))\n",
        "\n",
        "\n",
        "def preprocess_video(video_path, output_dir):\n",
        "    \"\"\"Full preprocessing pipeline for a single video\"\"\"\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    frame_dir = os.path.join(output_dir, video_name)\n",
        "\n",
        "    #  Skip if already processed\n",
        "    if os.path.exists(frame_dir) and any(f.startswith(\"seq_\") and f.endswith(\".npy\") for f in os.listdir(frame_dir)):\n",
        "        print(f\" Skipping already processed: {video_name}\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "    # Step 1: Process video\n",
        "    processor = VideoProcessor()\n",
        "    frames = processor.extract_frames(video_path)\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        print(f\" No valid frames found in: {video_name}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Save frames\n",
        "    for i, frame in enumerate(frames):\n",
        "        cv2.imwrite(f\"{frame_dir}/frame_{i:04d}.jpg\", frame)\n",
        "\n",
        "    # Step 3: Create sequences\n",
        "    seq_generator = SequenceGenerator()\n",
        "    sequences = seq_generator.create_sequences(frames)\n",
        "\n",
        "    # Step 4: Save sequences\n",
        "    for i, seq in enumerate(sequences):\n",
        "        normalized_seq = seq_generator.normalize(seq)\n",
        "        np.save(f\"{frame_dir}/seq_{i:03d}.npy\", normalized_seq)\n",
        "\n",
        "    return len(sequences)\n",
        "\n",
        "\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.sequence_paths = self._find_sequences()\n",
        "\n",
        "    def _find_sequences(self):\n",
        "        sequence_paths = []\n",
        "        for root, _, files in os.walk(self.data_dir):\n",
        "            for file in files:\n",
        "                if file.startswith('seq_') and file.endswith('.npy'):\n",
        "                    sequence_paths.append(os.path.join(root, file))\n",
        "        return sequence_paths\n",
        "\n",
        "    def _len_(self):\n",
        "        return len(self.sequence_paths)\n",
        "\n",
        "    def _getitem_(self, idx):\n",
        "        seq_path = self.sequence_paths[idx]\n",
        "        sequence = np.load(seq_path)\n",
        "\n",
        "        # Determine label from directory name\n",
        "        label = 0 if \"fake\" in seq_path.lower() else 1\n",
        "        return torch.tensor(sequence), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Model Architecture\n",
        "\n",
        "\n",
        "from timm import create_model\n",
        "import torch.nn as nn\n",
        "\n",
        "from timm import create_model\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = create_model(\n",
        "            'xception',\n",
        "            pretrained=True,\n",
        "            features_only=True,\n",
        "            out_indices=(3,)  # Choose the layer that gives you 728-dim features\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)  # Merge batch and time\n",
        "        features = self.backbone(x)[0]  # Get feature map\n",
        "        _, C_f, H_f, W_f = features.shape\n",
        "        return features.view(B, T, C_f, H_f, W_f)  # Unstack time\n",
        "\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, num_frames=6, spatial_size=19, token_dim=728):\n",
        "        super().__init__()\n",
        "        self.num_frames = num_frames\n",
        "        self.num_patches = spatial_size * spatial_size\n",
        "        self.token_dim = token_dim\n",
        "\n",
        "        # Classification tokens\n",
        "        self.spatial_cls = nn.Parameter(torch.randn(1, num_frames, 1, token_dim))\n",
        "        self.temporal_cls = nn.Parameter(torch.randn(1, 1, self.num_patches + 1, token_dim))\n",
        "\n",
        "        # Position embedding\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_frames + 1, self.num_patches + 1, token_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, T, C, H, W) -> Output: (B, T+1, HW+1, D)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.flatten(3).permute(0, 1, 3, 2)  # (B, T, HW, C)\n",
        "\n",
        "        # Add spatial CLS tokens\n",
        "        spatial_cls = self.spatial_cls.expand(B, -1, -1, -1)\n",
        "        x = torch.cat([spatial_cls, x], dim=2)  # (B, T, HW+1, D)\n",
        "\n",
        "        # Add temporal CLS token\n",
        "        temporal_cls = self.temporal_cls.expand(B, -1, -1, -1)\n",
        "        x = torch.cat([temporal_cls, x], dim=1)  # (B, T+1, HW+1, D)\n",
        "\n",
        "        # Add position embedding\n",
        "        x = x + self.pos_embed\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecomposedAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def temporal_attention(self, q, k, v):\n",
        "        # q, k, v: (B, N, S, T, D)\n",
        "        attn = torch.einsum('bnstd,bnsud->bnstu', q, k) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        return torch.einsum('bnstu,bnsud->bnstd', attn, v)\n",
        "\n",
        "    def spatial_attention(self, q, k, v):\n",
        "        # q, k, v: (B, N, T, S, D)\n",
        "        attn = torch.einsum('bntsd,bntud->bntsu', q, k) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        return torch.einsum('bntsu,bntud->bntsd', attn, v)\n",
        "\n",
        "    def forward(self, x, mode='temporal'):\n",
        "        B, T, S, D = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, T, S, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(3, 0, 4, 1, 2, 5)  # (3, B, N, T, S, D)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        if mode == 'temporal':\n",
        "            q = q.permute(0, 1, 3, 2, 4)  # (B, N, S, T, D)\n",
        "            k = k.permute(0, 1, 3, 2, 4)\n",
        "            v = v.permute(0, 1, 3, 2, 4)\n",
        "            out = self.temporal_attention(q, k, v).permute(0, 1, 3, 2, 4)\n",
        "        else:  # spatial\n",
        "            out = self.spatial_attention(q, k, v)\n",
        "\n",
        "        out = out.permute(0, 2, 3, 1, 4).reshape(B, T, S, D)\n",
        "        return self.proj(out)\n",
        "\n",
        "\n",
        "class SelfSubtract(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, S, D)\n",
        "        t_cls = x[:, 0:1]  # Temporal CLS token\n",
        "        frames = x[:, 1:]\n",
        "\n",
        "        # Compute residuals: [frame2-frame1, frame3-frame2, ...]\n",
        "        residuals = frames[:, 1:] - frames[:, :-1]\n",
        "\n",
        "        # Combine CLS, first frame, and residuals\n",
        "        return torch.cat([t_cls, frames[:, 0:1], residuals], dim=1)\n",
        "\n",
        "\n",
        "class ISTVTBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.temp_attn = DecomposedAttention(dim, num_heads)\n",
        "        self.spatial_attn = DecomposedAttention(dim, num_heads)\n",
        "        self.self_subtract = SelfSubtract()\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Temporal attention with self-subtract\n",
        "        x_norm = self.norm1(x)\n",
        "        x_sub = self.self_subtract(x_norm)\n",
        "        temp_out = self.temp_attn(x_sub, mode='temporal') + x\n",
        "\n",
        "        # Spatial attention\n",
        "        temp_norm = self.norm2(temp_out)\n",
        "        spatial_out = self.spatial_attn(temp_norm, mode='spatial') + temp_out\n",
        "\n",
        "        # FFN\n",
        "        ffn_in = self.norm3(spatial_out)\n",
        "        return self.ffn(ffn_in) + spatial_out\n",
        "\n",
        "\n",
        "class ISTVT(nn.Module):\n",
        "    def __init__(self, num_frames=6, num_blocks=12, dim=728, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.tokenizer = Tokenizer(num_frames=num_frames)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ISTVTBlock(dim, num_heads) for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, T, C, H, W) = (batch, 6, 3, 300, 300)\n",
        "        features = self.feature_extractor(x)  # (B, 6, 728, 19, 19)\n",
        "        tokens = self.tokenizer(features)     # (B, 7, 362, 728)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            tokens = block(tokens)\n",
        "\n",
        "        # Use temporal CLS token for prediction\n",
        "        cls_token = tokens[:, 0, 0]\n",
        "        return self.head(cls_token).squeeze(1)\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.sequence_paths = self._find_sequences()\n",
        "\n",
        "    def _find_sequences(self):\n",
        "        sequence_paths = []\n",
        "        for root, _, files in os.walk(self.data_dir):\n",
        "            for file in files:\n",
        "                if file.startswith('seq_') and file.endswith('.npy'):\n",
        "                    sequence_paths.append(os.path.join(root, file))\n",
        "        return sequence_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_path = self.sequence_paths[idx]\n",
        "        sequence = np.load(seq_path)\n",
        "\n",
        "        # Determine label from directory name\n",
        "        label = 0 if \"fake\" in seq_path.lower() else 1\n",
        "        return torch.tensor(sequence), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Training Setup\n",
        "\n",
        "def train(model, dataloader, val_loader, epochs=100, lr=0.0005, checkpoint_dir=\"checkpoints\"):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    start_epoch = 0\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Resume if last_epoch.txt exists\n",
        "    last_epoch_file = os.path.join(checkpoint_dir, \"last_epoch.txt\")\n",
        "    if os.path.exists(last_epoch_file):\n",
        "        with open(last_epoch_file, \"r\") as f:\n",
        "            start_epoch = int(f.read().strip()) + 1\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{start_epoch - 1}.pth\")\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            model.load_state_dict(torch.load(checkpoint_path))\n",
        "            print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "        for sequences, labels in progress:\n",
        "            sequences = sequences.to(device).float()\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_loader:\n",
        "                sequences = sequences.to(device).float()\n",
        "                labels = labels.to(device).float()\n",
        "\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        train_loss /= len(dataloader)\n",
        "        val_loss /= len(val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Save current epoch\n",
        "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pth\"))\n",
        "        with open(last_epoch_file, \"w\") as f:\n",
        "            f.write(str(epoch))\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
        "            print(\"Saved best model!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Main Execution\n",
        "# ======================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    import os\n",
        "\n",
        "    # === Configuration ===\n",
        "    DATA_DIR = \"/content/drive/MyDrive/ISTVT Dataset\"  # This should contain 'real/' and 'fake/'\n",
        "    PROCESSED_DIR = \"processed_data\"\n",
        "    CHECKPOINT_DIR = \"/content/drive/MyDrive/deepfake_checkpoints\"\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "    EPOCHS = 5\n",
        "    NUM_FRAMES = 6\n",
        "\n",
        "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # === Step 1: Preprocess videos (real + fake) ===\n",
        "    for label in [\"real\", \"fake\"]:\n",
        "        input_dir = os.path.join(DATA_DIR, label)\n",
        "        output_dir = os.path.join(PROCESSED_DIR, label)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        for video_file in os.listdir(input_dir):\n",
        "            if video_file.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "                video_path = os.path.join(input_dir, video_file)\n",
        "                print(f\"Preprocessing {video_path}...\")\n",
        "                preprocess_video(video_path, output_dir)\n",
        "\n",
        "    # === Step 2: Load dataset ===\n",
        "    full_dataset = DeepfakeDataset(PROCESSED_DIR)\n",
        "    print (\"lenght of dataset is \": {len(full_dataset)})\n",
        "\n",
        "    # === Step 3: Split into train/val ===\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    # === Step 4: Initialize Model ===\n",
        "    model = ISTVT(num_frames=NUM_FRAMES)\n",
        "\n",
        "    # === Step 5: Train (auto resume + save to Drive) ===\n",
        "    trained_model = train(model, train_loader, val_loader, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n",
        "\n",
        "    print(\" Training completed and model saved in Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 6: Evaluate a Single Video ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from facenet_pytorch import MTCNN\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from timm import create_model\n",
        "\n",
        "def evaluate_single_video(model, video_path, num_frames=6):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\n📹 Evaluating video: {os.path.basename(video_path)}\")\n",
        "\n",
        "    # Extract and process frames\n",
        "    frames = VideoProcessor(num_frames=num_frames).extract_frames(video_path)\n",
        "    if len(frames) < num_frames:\n",
        "        print(\" Not enough frames for evaluation.\")\n",
        "        return\n",
        "\n",
        "    # Create sequences\n",
        "    seq_gen = SequenceGenerator(seq_length=num_frames)\n",
        "    sequences = seq_gen.create_sequences(frames)\n",
        "\n",
        "    # Predict on all sequences\n",
        "    preds = []\n",
        "    for seq in sequences:\n",
        "        norm_seq = seq_gen.normalize(seq)\n",
        "        tensor = torch.tensor(norm_seq).unsqueeze(0).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(tensor)\n",
        "            prob = torch.sigmoid(output).item()\n",
        "            preds.append(prob)\n",
        "\n",
        "    avg_prob = np.mean(preds)\n",
        "    pred_label = int(avg_prob > 0.5)\n",
        "\n",
        "    print(f\"\\n Prediction: {'REAL' if pred_label else 'FAKE'}\")\n",
        "    print(f\" Confidence Score: {avg_prob:.4f}\")\n",
        "\n",
        "# === Example usage ===\n",
        "trained_model = \"/content/best_istvt_model.pth\"\n",
        "TEST_VIDEO_PATH = \"/content/01_03__talking_against_wall__JZUXXFRB.mp4\"  # change this to your test video path\n",
        "\n",
        "evaluate_single_video(trained_model, TEST_VIDEO_PATH, num_frames=6)\n"
      ],
      "metadata": {
        "id": "RYqxCsSG0lBR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 6: Evaluate a Single Video ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from facenet_pytorch import MTCNN\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from timm import create_model\n",
        "\n",
        "def evaluate_single_video(model, video_path, num_frames=6):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\n Evaluating video: {os.path.basename(video_path)}\")\n",
        "\n",
        "    # Extract and process frames\n",
        "    frames = VideoProcessor(num_frames=num_frames).extract_frames(video_path)\n",
        "    if len(frames) < num_frames:\n",
        "        print(\" Not enough frames for evaluation.\")\n",
        "        return\n",
        "\n",
        "    # Create sequences\n",
        "    seq_gen = SequenceGenerator(seq_length=num_frames)\n",
        "    sequences = seq_gen.create_sequences(frames)\n",
        "\n",
        "    # Predict on all sequences\n",
        "    preds = []\n",
        "    for seq in sequences:\n",
        "        norm_seq = seq_gen.normalize(seq)\n",
        "        tensor = torch.tensor(norm_seq).unsqueeze(0).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(tensor)\n",
        "            prob = torch.sigmoid(output).item()\n",
        "            preds.append(prob)\n",
        "\n",
        "    avg_prob = np.mean(preds)\n",
        "    pred_label = int(avg_prob > 0.5)\n",
        "\n",
        "    print(f\"\\n Prediction: {'REAL' if pred_label else 'FAKE'}\")\n",
        "    print(f\"Confidence Score: {avg_prob:.4f}\")\n",
        "\n",
        "# === Example usage ===\n",
        "trained_model = \"/content/best_istvt_model.pth\"\n",
        "TEST_VIDEO_PATH = \"/content/01__meeting_serious.mp4\"  # change this to your test video path\n",
        "\n",
        "evaluate_single_video(trained_model, TEST_VIDEO_PATH, num_frames=6)\n"
      ],
      "metadata": {
        "id": "RDZkj0Itq_1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
